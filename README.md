<div align="center">
    <img height=200 src="./.github/images/news-logo.png" alt="News Contents on Smartphone">
</div>

<h1 align="center">ğŸ“° Há»‡ thá»‘ng Gá»£i Ã½ Tin tá»©c sá»­ dá»¥ng LLM</h1>
<p align="center"><strong>News Recommendation System sá»­ dá»¥ng Pre-trained Large Language Model (BERT/DistilBERT) vá»›i PyTorch ğŸš€</strong></p>

---

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t nhanh](#-cÃ i-Ä‘áº·t-nhanh)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
  - [1. Táº£i dataset](#1-táº£i-dataset)
  - [2. Train model](#2-train-model)
  - [3. Test/Evaluate model](#3-testevaluate-model)
- [Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [Cáº¥u hÃ¬nh](#-cáº¥u-hÃ¬nh)
- [Káº¿t quáº£ thá»±c nghiá»‡m](#-káº¿t-quáº£-thá»±c-nghiá»‡m)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n nÃ y triá»ƒn khai há»‡ thá»‘ng gá»£i Ã½ tin tá»©c sá»­ dá»¥ng **Neural News Recommendation with Multi-Head Self-Attention (NRMS)** káº¿t há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° **BERT** vÃ  **DistilBERT**.

### âœ¨ TÃ­nh nÄƒng chÃ­nh

- âœ… Sá»­ dá»¥ng Pre-trained Language Models (BERT/DistilBERT) Ä‘á»ƒ mÃ£ hÃ³a ná»™i dung tin tá»©c
- âœ… MÃ´ hÃ¬nh NRMS vá»›i Multi-Head Self-Attention
- âœ… Há»— trá»£ dataset MIND (Microsoft News Dataset)
- âœ… ÄÃ¡nh giÃ¡ vá»›i cÃ¡c metrics: AUC, MRR, nDCG@5, nDCG@10

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n má»m cáº§n thiáº¿t

- **Python**: 3.11.3
- **PyTorch**: 2.0.1+
- **CUDA**: Khuyáº¿n nghá»‹ (náº¿u cÃ³ GPU)
- **Rye**: Package manager (hoáº·c pip)

### Pháº§n cá»©ng khuyáº¿n nghá»‹

- **GPU**: NVIDIA GPU vá»›i CUDA support (khuyáº¿n nghá»‹ cho training)
- **RAM**: Tá»‘i thiá»ƒu 8GB, khuyáº¿n nghá»‹ 16GB+
- **Disk**: Tá»‘i thiá»ƒu 10GB cho dataset vÃ  model

---

## ğŸš€ CÃ i Ä‘áº·t nhanh

### BÆ°á»›c 1: Clone repository

```bash
git clone <repository-url>
cd kaggle-news-recommendation
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t dependencies

**CÃ¡ch 1: Sá»­ dá»¥ng Rye (khuyáº¿n nghá»‹)**

```bash
# CÃ i Ä‘áº·t Rye (náº¿u chÆ°a cÃ³)
curl -sSf https://rye-up.com/get | bash

# Äá»“ng bá»™ dependencies
rye sync
```

**CÃ¡ch 2: Sá»­ dá»¥ng pip**

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 3: Thiáº¿t láº­p mÃ´i trÆ°á»ng

```bash
# Thiáº¿t láº­p PYTHONPATH
export PYTHONPATH=$(pwd)/src:$PYTHONPATH

# Hoáº·c thÃªm vÃ o ~/.bashrc hoáº·c ~/.zshrc Ä‘á»ƒ tá»± Ä‘á»™ng load
echo 'export PYTHONPATH=$(pwd)/src:$PYTHONPATH' >> ~/.bashrc
```

### BÆ°á»›c 4: Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
# Kiá»ƒm tra Python version
python --version  # NÃªn lÃ  3.11.3

# Kiá»ƒm tra PyTorch vÃ  CUDA
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
```

---

## ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1. Táº£i Dataset

TrÆ°á»›c khi train, báº¡n cáº§n táº£i dataset MIND:

```bash
# Sá»­ dá»¥ng Rye
rye run python ./dataset/download_mind.py

# Hoáº·c sá»­ dá»¥ng Python trá»±c tiáº¿p (sau khi Ä‘Ã£ set PYTHONPATH)
python ./dataset/download_mind.py
```

**LÆ°u Ã½**: QuÃ¡ trÃ¬nh download cÃ³ thá»ƒ máº¥t vÃ i phÃºt tÃ¹y vÃ o tá»‘c Ä‘á»™ máº¡ng.

Sau khi táº£i xong, cáº¥u trÃºc thÆ° má»¥c sáº½ nhÆ° sau:

```
dataset/
â””â”€â”€ mind/
    â”œâ”€â”€ small/
    â”‚   â”œâ”€â”€ train/
    â”‚   â”‚   â”œâ”€â”€ behaviors.tsv
    â”‚   â”‚   â””â”€â”€ news.tsv
    â”‚   â””â”€â”€ val/
    â”‚       â”œâ”€â”€ behaviors.tsv
    â”‚       â””â”€â”€ news.tsv
    â””â”€â”€ large/
        â”œâ”€â”€ train/
        â”œâ”€â”€ val/
        â””â”€â”€ test/
```

---

### 2. Train Model

#### ğŸ¯ Train vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh

CÃ¡ch Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ báº¯t Ä‘áº§u train:

```bash
# Sá»­ dá»¥ng Rye
rye run python src/experiment/train.py

# Hoáº·c sá»­ dá»¥ng Python trá»±c tiáº¿p
python src/experiment/train.py
```

#### âš™ï¸ Train vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh

Báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh cÃ¡c hyperparameters qua command line:

```bash
rye run python src/experiment/train.py \
    random_seed=42 \
    pretrained="distilbert-base-uncased" \
    npratio=4 \
    history_size=50 \
    batch_size=16 \
    gradient_accumulation_steps=8 \
    epochs=3 \
    learning_rate=1e-4 \
    weight_decay=0.0 \
    max_len=30
```

#### ğŸ“ Giáº£i thÃ­ch cÃ¡c tham sá»‘

| Tham sá»‘                       | MÃ´ táº£                                     | GiÃ¡ trá»‹ máº·c Ä‘á»‹nh            |
| ----------------------------- | ----------------------------------------- | --------------------------- |
| `random_seed`                 | Seed cho reproducibility                  | `42`                        |
| `pretrained`                  | TÃªn mÃ´ hÃ¬nh pre-trained                   | `"distilbert-base-uncased"` |
| `npratio`                     | Tá»· lá»‡ negative sampling                   | `4`                         |
| `history_size`                | Sá»‘ lÆ°á»£ng tin tá»©c trong lá»‹ch sá»­ ngÆ°á»i dÃ¹ng | `50`                        |
| `batch_size`                  | Batch size cho training                   | `16`                        |
| `gradient_accumulation_steps` | Sá»‘ bÆ°á»›c tÃ­ch lÅ©y gradient                 | `8`                         |
| `epochs`                      | Sá»‘ epochs Ä‘á»ƒ train                        | `1`                         |
| `learning_rate`               | Learning rate                             | `1e-4`                      |
| `weight_decay`                | Weight decay cho regularization           | `0.0`                       |
| `max_len`                     | Äá»™ dÃ i tá»‘i Ä‘a cá»§a sequence                | `30`                        |

**LÆ°u Ã½**:

- Batch size thá»±c táº¿ = `batch_size Ã— gradient_accumulation_steps` = 16 Ã— 8 = 128
- Model sáº½ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng trong `output/model/` vá»›i timestamp

#### ğŸ“Š Theo dÃµi quÃ¡ trÃ¬nh training

- Logs Ä‘Æ°á»£c lÆ°u trong `output/log/`
- Model checkpoints Ä‘Æ°á»£c lÆ°u trong `output/model/YYYY-MM-DD/HH-MM-SS/checkpoint-{step}/`
- Má»—i checkpoint chá»©a:
  - `model.safetensors`: Trá»ng sá»‘ cá»§a model
  - `optimizer.pt`: Tráº¡ng thÃ¡i optimizer
  - `scheduler.pt`: Tráº¡ng thÃ¡i learning rate scheduler
  - `trainer_state.json`: Tráº¡ng thÃ¡i trainer

---

### 3. Test/Evaluate Model

#### ğŸ§ª ÄÃ¡nh giÃ¡ trÃªn Test Dataset

Sau khi train xong, báº¡n cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ model trÃªn test dataset:

```bash
# Sá»­ dá»¥ng model má»›i nháº¥t (tá»± Ä‘á»™ng tÃ¬m)
rye run python src/experiment/evaluate.py

# Hoáº·c chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n model cá»¥ thá»ƒ
rye run python src/experiment/evaluate.py \
    model_path="output/model/2025-11-28/09-06-09/checkpoint-614" \
    pretrained="distilbert-base-uncased" \
    history_size=50 \
    max_len=30
```

#### ğŸ“ˆ CÃ¡c metrics Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡

- **AUC**: Area Under the ROC Curve
- **MRR**: Mean Reciprocal Rank
- **nDCG@5**: Normalized Discounted Cumulative Gain táº¡i top 5
- **nDCG@10**: Normalized Discounted Cumulative Gain táº¡i top 10

#### ğŸ² So sÃ¡nh vá»›i Random Baseline

Báº¡n cÅ©ng cÃ³ thá»ƒ cháº¡y random baseline Ä‘á»ƒ so sÃ¡nh:

```bash
rye run python src/experiment/evaluate_random.py
```

---

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
kaggle-news-recommendation/
â”œâ”€â”€ ğŸ“‚ dataset/                    # ThÆ° má»¥c chá»©a dataset
â”‚   â”œâ”€â”€ download_mind.py          # Script táº£i MIND dataset
â”‚   â””â”€â”€ mind/                      # Dataset MIND sau khi táº£i
â”‚       â”œâ”€â”€ small/                 # MIND Small dataset
â”‚       â””â”€â”€ large/                 # MIND Large dataset
â”‚
â”œâ”€â”€ ğŸ“‚ src/                        # Source code chÃ­nh
â”‚   â”œâ”€â”€ ğŸ“‚ config/                 # Cáº¥u hÃ¬nh
â”‚   â”‚   â””â”€â”€ config.py             # TrainConfig vÃ  EvalConfig
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ const/                  # Constants
â”‚   â”‚   â””â”€â”€ path.py               # ÄÆ°á»ng dáº«n cÃ¡c thÆ° má»¥c
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/            # ÄÃ¡nh giÃ¡ model
â”‚   â”‚   â””â”€â”€ RecEvaluator.py       # Metrics: AUC, MRR, nDCG
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ experiment/            # Scripts train vÃ  evaluate
â”‚   â”‚   â”œâ”€â”€ train.py              # ğŸš‚ Script training
â”‚   â”‚   â”œâ”€â”€ evaluate.py           # ğŸ§ª Script evaluation
â”‚   â”‚   â””â”€â”€ evaluate_random.py    # Random baseline
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ mind/                  # Xá»­ lÃ½ MIND dataset
â”‚   â”‚   â”œâ”€â”€ dataframe.py          # Äá»c dá»¯ liá»‡u tá»« TSV
â”‚   â”‚   â””â”€â”€ MINDDataset.py        # Dataset class cho PyTorch
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ recommendation/        # MÃ´ hÃ¬nh recommendation
â”‚   â”‚   â””â”€â”€ ğŸ“‚ nrms/              # NRMS model
â”‚   â”‚       â”œâ”€â”€ NRMS.py           # Model chÃ­nh
â”‚   â”‚       â”œâ”€â”€ PLMBasedNewsEncoder.py  # Encoder cho tin tá»©c
â”‚   â”‚       â”œâ”€â”€ UserEncoder.py    # Encoder cho ngÆ°á»i dÃ¹ng
â”‚   â”‚       â””â”€â”€ AdditiveAttention.py   # Attention mechanism
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                 # Utilities
â”‚       â”œâ”€â”€ logger.py             # Logging
â”‚       â”œâ”€â”€ path.py               # Path utilities
â”‚       â”œâ”€â”€ random_seed.py        # Set random seed
â”‚       â””â”€â”€ text.py               # Text processing
â”‚
â”œâ”€â”€ ğŸ“‚ output/                     # Output files
â”‚   â”œâ”€â”€ ğŸ“‚ model/                 # Saved models
â”‚   â””â”€â”€ ğŸ“‚ log/                   # Training logs
â”‚
â”œâ”€â”€ ğŸ“‚ test/                       # Unit tests
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ pyproject.toml             # Rye configuration
â””â”€â”€ ğŸ“„ README.md                  # File nÃ y
```

---

## âš™ï¸ Cáº¥u hÃ¬nh

### Cáº¥u hÃ¬nh Training

File cáº¥u hÃ¬nh: `src/config/config.py`

```python
@dataclass
class TrainConfig:
    random_seed: int = 42
    pretrained: str = "distilbert-base-uncased"  # hoáº·c "bert-base-uncased"
    npratio: int = 4
    history_size: int = 50
    batch_size: int = 16
    gradient_accumulation_steps: int = 8
    epochs: int = 1
    learning_rate: float = 1e-4
    weight_decay: float = 0.0
    max_len: int = 30
```

### Cáº¥u hÃ¬nh Evaluation

```python
@dataclass
class EvalConfig:
    random_seed: int = 42
    pretrained: str = "distilbert-base-uncased"
    history_size: int = 50
    max_len: int = 30
    model_path: str = ""  # Äá»ƒ trá»‘ng sáº½ dÃ¹ng model má»›i nháº¥t
```

---

## ğŸ“Š Káº¿t quáº£ thá»±c nghiá»‡m

### Káº¿t quáº£ trÃªn MIND Small Dataset

| Model                      | AUC       | MRR       | nDCG@5    | nDCG@10   | Thá»i gian train |
| -------------------------- | --------- | --------- | --------- | --------- | --------------- |
| Random Recommendation      | 0.500     | 0.201     | 0.203     | 0.267     | -               |
| **NRMS + DistilBERT-base** | **0.674** | **0.297** | **0.322** | **0.387** | **15.0h**       |
| **NRMS + BERT-base**       | **0.689** | **0.306** | **0.336** | **0.400** | **28.5h**       |

_Káº¿t quáº£ Ä‘Æ°á»£c Ä‘o trÃªn Single GPU (V100 x 1)_

### Model Ä‘Ã£ Ä‘Æ°á»£c train sáºµn

Náº¿u báº¡n muá»‘n sá»­ dá»¥ng model Ä‘Ã£ Ä‘Æ°á»£c train sáºµn:

| Model                  | Link                                                                                               |
| ---------------------- | -------------------------------------------------------------------------------------------------- |
| NRMS + DistilBERT-base | [Google Drive](https://drive.google.com/file/d/1cw9WQSOVYJdYJCuIrSmU8odV2nsmith5/view?usp=sharing) |
| NRMS + BERT-base       | [Google Drive](https://drive.google.com/file/d/1ARiUgSVwcDFopFoIusp2MGQzwTMncOFf/view?usp=sharing) |

---

## ğŸ”§ Troubleshooting

### âŒ Lá»—i: `ModuleNotFoundError`

**NguyÃªn nhÃ¢n**: ChÆ°a set PYTHONPATH

**Giáº£i phÃ¡p**:

```bash
export PYTHONPATH=$(pwd)/src:$PYTHONPATH
```

### âŒ Lá»—i: `CUDA out of memory`

**NguyÃªn nhÃ¢n**: GPU khÃ´ng Ä‘á»§ bá»™ nhá»›

**Giáº£i phÃ¡p**:

- Giáº£m `batch_size` (vÃ­ dá»¥: tá»« 16 xuá»‘ng 8)
- TÄƒng `gradient_accumulation_steps` Ä‘á»ƒ giá»¯ nguyÃªn effective batch size
- Sá»­ dá»¥ng `torch.bfloat16` (Ä‘Ã£ Ä‘Æ°á»£c enable máº·c Ä‘á»‹nh)

### âŒ Lá»—i: Dataset khÃ´ng tÃ¬m tháº¥y

**NguyÃªn nhÃ¢n**: ChÆ°a táº£i dataset hoáº·c Ä‘Æ°á»ng dáº«n sai

**Giáº£i phÃ¡p**:

```bash
# Kiá»ƒm tra dataset Ä‘Ã£ táº£i chÆ°a
ls dataset/mind/small/train/

# Náº¿u chÆ°a cÃ³, cháº¡y láº¡i script download
python ./dataset/download_mind.py
```

### âŒ Lá»—i: Model checkpoint khÃ´ng tÃ¬m tháº¥y khi evaluate

**NguyÃªn nhÃ¢n**: ÄÆ°á»ng dáº«n model sai hoáº·c chÆ°a train

**Giáº£i phÃ¡p**:

- Kiá»ƒm tra model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong `output/model/`
- Chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ trong config hoáº·c command line
- Äáº£m báº£o checkpoint cÃ³ file `model.safetensors` hoáº·c `pytorch_model.bin`

### âš ï¸ Training cháº­m

**CÃ¡c cÃ¡ch tá»‘i Æ°u**:

- Sá»­ dá»¥ng GPU thay vÃ¬ CPU
- Sá»­ dá»¥ng `distilbert-base-uncased` thay vÃ¬ `bert-base-uncased` (nhanh hÆ¡n ~2x)
- TÄƒng `batch_size` vÃ  `gradient_accumulation_steps` náº¿u GPU Ä‘á»§ máº¡nh

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Papers

1. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**

   - Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K.
   - https://aclanthology.org/N19-1423

2. **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**

   - Sanh, V., Debut, L., Chaumond, J., & Wolf, T.
   - https://arxiv.org/abs/1910.01108

3. **Neural News Recommendation with Multi-Head Self-Attention**

   - Wu, C., Wu, F., Ge, S., Qi, T., Huang, Y., & Xie, X.
   - https://aclanthology.org/D19-1671

4. **Empowering News Recommendation with Pre-Trained Language Models**

   - Wu, C., Wu, F., Qi, T., & Huang, Y.
   - https://doi.org/10.1145/3404835.3463069

5. **MIND: A Large-scale Dataset for News Recommendation**
   - Wu, F., Qiao, Y., Chen, J.-H., Wu, C., Qi, T., Lian, J., Liu, D., Xie, X., Gao, J., Wu, W., & Zhou, M.
   - https://aclanthology.org/2020.acl-main.331

---

## ğŸ“ Citation

Náº¿u báº¡n sá»­ dá»¥ng code nÃ y trong nghiÃªn cá»©u, vui lÃ²ng cite:

```bibtex
@misc{yuki-yada-news-rec,
  author = {Yuki Yada},
  title = {News Recommendation using PLMs},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/YadaYuki/news-recommendation-llm}}
}
```

---

## ğŸ¤ ÄÃ³ng gÃ³p

ChÃºng tÃ´i hoan nghÃªnh má»i Ä‘Ã³ng gÃ³p! Náº¿u báº¡n muá»‘n sá»­ dá»¥ng pháº§n má»m nÃ y trong nghiÃªn cá»©u hoáº·c dá»± Ã¡n, vui lÃ²ng liÃªn há»‡: yada.yuki@fuji.waseda.jp

---

## ğŸ“„ License

Xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

<div align="center">
    <p>â­ Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y star Ä‘á»ƒ á»§ng há»™! â­</p>
</div>
